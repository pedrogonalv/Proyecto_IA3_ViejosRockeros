# Sistema RAG para DocumentaciÃ³n TÃ©cnica

Sistema de RecuperaciÃ³n Aumentada por GeneraciÃ³n (RAG) especializado en el procesamiento inteligente de manuales tÃ©cnicos en PDF, con capacidades de extracciÃ³n multimodal, bÃºsqueda semÃ¡ntica y generaciÃ³n automÃ¡tica de datasets Q&A.

## ğŸš€ CaracterÃ­sticas Principales

### Procesamiento de Documentos
- **Procesamiento Adaptativo**: Detecta automÃ¡ticamente el tipo de documento (tÃ©cnico, texto, escaneado, mixto) y aplica la estrategia Ã³ptima
- **ExtracciÃ³n Multimodal**: 
  - Texto con chunking inteligente y preservaciÃ³n de contexto
  - Tablas con preservaciÃ³n de estructura (exportaciÃ³n a CSV)
  - ImÃ¡genes raster embebidas con metadatos
  - Diagramas tÃ©cnicos renderizados de alta calidad
  - OCR integrado para documentos escaneados (Tesseract)
- **OrganizaciÃ³n Inteligente**: Estructura de carpetas automÃ¡tica por manual con separaciÃ³n por tipo de contenido

### Almacenamiento y BÃºsqueda
- **Almacenamiento HÃ­brido**: 
  - SQLite para metadatos estructurados y bÃºsqueda rÃ¡pida
  - ChromaDB para embeddings vectoriales
  - Sistema de archivos para contenido binario (imÃ¡genes, CSVs)
- **BÃºsqueda HÃ­brida**: Combina bÃºsqueda vectorial semÃ¡ntica con bÃºsqueda por palabras clave (FTS5)
- **Cache Inteligente**: LMDB para embeddings y resultados de bÃºsqueda frecuentes

### GeneraciÃ³n de Datasets Q&A
- **GeneraciÃ³n AutomÃ¡tica**: Crea pares pregunta-respuesta de alta calidad desde chunks procesados
- **MÃºltiples Tipos de Preguntas**: Factual, SÃ­ntesis, Causal, AplicaciÃ³n, AnÃ¡lisis
- **Control de Calidad**: ValidaciÃ³n automÃ¡tica y filtrado de relevancia
- **Procesamiento Masivo**: Manejo eficiente con rate limiting y reanudaciÃ³n automÃ¡tica

### Soporte Multiidioma
- Optimizado para documentaciÃ³n tÃ©cnica en **espaÃ±ol** e **inglÃ©s**
- Modelo de embeddings multilingÃ¼e de alto rendimiento

## ğŸ“‹ Requisitos del Sistema

### Hardware
- **CPU**: 4+ cores recomendado
- **RAM**: 8GB mÃ­nimo (16GB recomendado para procesamiento masivo)
- **Disco**: 10GB+ espacio libre para procesamiento y almacenamiento
- **GPU**: Opcional, mejora el rendimiento de embeddings

### Software
- **Python**: 3.8+ (probado con 3.12)
- **Sistema Operativo**: macOS, Linux, Windows

### Dependencias del Sistema

#### Tesseract OCR (opcional, para documentos escaneados)
```bash
# macOS
brew install tesseract

# Ubuntu/Debian
sudo apt-get install tesseract-ocr tesseract-ocr-spa

# Windows
# Descargar desde: https://github.com/UB-Mannheim/tesseract/wiki
```

#### Java (opcional, para extracciÃ³n avanzada de tablas con Tabula)
```bash
# Verificar instalaciÃ³n
java -version
```

## ğŸ› ï¸ InstalaciÃ³n

### 1. Clonar el repositorio
```bash
git clone <repository-url>
cd clode_technical_rag_system
```

### 2. Crear y activar entorno virtual
```bash
python -m venv venv_rag_clean
source venv_rag_clean/bin/activate  # En Windows: venv_rag_clean\Scripts\activate
```

### 3. Instalar dependencias
```bash
pip install -r requirements.txt
```

### 4. Inicializar el sistema
```bash
python scripts/init_system.py
```

Este script automÃ¡ticamente:
- âœ“ Verifica versiÃ³n de Python y todas las dependencias
- âœ“ Detecta dependencias del sistema (Tesseract, Java)
- âœ“ Crea la estructura completa de directorios
- âœ“ Inicializa la base de datos SQLite con esquema actualizado
- âœ“ Genera un reporte detallado de inicializaciÃ³n

### 5. Configurar variables de entorno (opcional)
```bash
cp .env.example .env
# Editar .env segÃºn necesidades
```

Variables disponibles:
- `OPENAI_API_KEY`: Para generaciÃ³n de Q&A (opcional)
- `LOG_LEVEL`: DEBUG, INFO, WARNING, ERROR
- `MAX_WORKERS`: NÃºmero de workers paralelos
- `BATCH_SIZE`: TamaÃ±o de lote para procesamiento

## ğŸ“š Uso del Sistema

### 1. Procesamiento de Manuales PDF

#### Procesamiento bÃ¡sico
```bash
# Procesar todos los PDFs en el directorio por defecto
python scripts/process_manuals_sqlite.py --pdf-dir data/raw_pdfs/

# Procesar un solo PDF
python scripts/process_manuals_sqlite.py --single-pdf manual.pdf

# Procesar con generaciÃ³n de embeddings
python scripts/process_manuals_sqlite.py --pdf-dir data/raw_pdfs/ --embeddings
```

#### Procesamiento con metadatos especÃ­ficos
```bash
python scripts/process_manuals_sqlite.py --single-pdf manual.pdf \
  --manufacturer "Beckhoff" \
  --model "AX5000" \
  --doc-type technical \
  --embeddings
```

#### Reprocesamiento selectivo
```bash
# Reprocesar solo imÃ¡genes y tablas para un manual
python scripts/process_manuals_sqlite.py --reprocess 1 --steps images tables

# Reprocesar todo para un manual especÃ­fico
python scripts/process_manuals_sqlite.py --reprocess 1 \
  --steps analysis chunks images tables embeddings
```

### 2. ConstrucciÃ³n de Base de Datos Vectorial

```bash
# Construir base vectorial desde SQLite
python scripts/build_vectordb_sqlite.py

# Forzar reconstrucciÃ³n completa
python scripts/build_vectordb_sqlite.py --force

# Procesar solo un manual especÃ­fico
python scripts/build_vectordb_sqlite.py --manual-id 1

# Ver estadÃ­sticas actuales
python scripts/build_vectordb_sqlite.py --stats

# Verificar sincronizaciÃ³n SQLite â†” ChromaDB
python scripts/build_vectordb_sqlite.py --verify
```

### 3. ExtracciÃ³n de Contenido Visual

```bash
# Extraer y organizar todo el contenido visual
python scripts/extract_with_manual_folders.py

# Procesar diagramas tÃ©cnicos especÃ­ficamente
python scripts/process_technical_diagrams.py data/raw_pdfs/ --batch
```

### 4. GeneraciÃ³n de Dataset Q&A

```bash
# Generar dataset completo (requiere OPENAI_API_KEY)
cd qa_generator
python process_all_chunks_v4.py \
    --model gpt-3.5-turbo \
    --batch-size 3 \
    --rps 2 \
    --output-file qa_dataset.jsonl
```

### 5. VerificaciÃ³n del Sistema

```bash
# Verificar integridad del sistema
python scripts/verify_system.py

# Test de extracciÃ³n de metadatos
python scripts/test_metadata_extraction.py
```

## ğŸ“ Estructura del Proyecto

```
clode_technical_rag_system/
â”œâ”€â”€ config/                      # ConfiguraciÃ³n del sistema
â”‚   â””â”€â”€ settings.py             # ConfiguraciÃ³n centralizada
â”œâ”€â”€ core/                       # Componentes principales
â”‚   â”œâ”€â”€ embedding_pipeline.py   # Pipeline de generaciÃ³n de embeddings
â”‚   â”œâ”€â”€ hybrid_search.py        # BÃºsqueda hÃ­brida (vectorial + keyword)
â”‚   â”œâ”€â”€ intelligent_chunking.py # Chunking semÃ¡ntico inteligente
â”‚   â””â”€â”€ unified_architecture.py # Arquitectura unificada del sistema
â”œâ”€â”€ data/                       # Almacenamiento de datos
â”‚   â”œâ”€â”€ raw_pdfs/              # PDFs originales a procesar
â”‚   â”œâ”€â”€ processed/             # Contenido procesado por manual
â”‚   â”‚   â””â”€â”€ [Manual_Name]/     
â”‚   â”‚       â”œâ”€â”€ images/        # ImÃ¡genes raster extraÃ­das
â”‚   â”‚       â”œâ”€â”€ diagrams/      # Diagramas tÃ©cnicos renderizados
â”‚   â”‚       â””â”€â”€ tables/        # Tablas en formato CSV
â”‚   â”œâ”€â”€ cache/                 # Sistemas de cache
â”‚   â”‚   â”œâ”€â”€ embedding_cache/   # Cache LMDB de embeddings
â”‚   â”‚   â””â”€â”€ search_cache/      # Cache de resultados de bÃºsqueda
â”‚   â”œâ”€â”€ sqlite/                # Base de datos SQLite
â”‚   â”‚   â””â”€â”€ manuals.db        # BD principal con toda la metadata
â”‚   â”œâ”€â”€ vectordb/             # Base de datos vectorial ChromaDB
â”‚   â””â”€â”€ logs/                 # Logs del sistema
â”‚       â”œâ”€â”€ processing/       # Logs detallados por manual
â”‚       â””â”€â”€ initialization_report.json
â”œâ”€â”€ database/                  # GestiÃ³n de base de datos
â”‚   â”œâ”€â”€ schema.sql            # Esquema SQLite actualizado
â”‚   â””â”€â”€ sqlite_manager.py     # Manager de operaciones SQLite
â”œâ”€â”€ extractors/               # Extractores de contenido
â”‚   â”œâ”€â”€ adaptive_processor.py # Procesador adaptativo por tipo
â”‚   â”œâ”€â”€ document_analyzer.py  # Analizador de tipo de documento
â”‚   â”œâ”€â”€ pdf_extractor.py      # Extractor de texto (PyMuPDF)
â”‚   â”œâ”€â”€ table_extractor.py    # Extractor de tablas (pdfplumber)
â”‚   â”œâ”€â”€ enhanced_image_extractor.py # Extractor de imÃ¡genes con OCR
â”‚   â””â”€â”€ sqlite_extractors.py  # Extractores integrados con SQLite
â”œâ”€â”€ models/                   # Modelos y embeddings
â”‚   â””â”€â”€ embeddings.py         # GestiÃ³n de modelos de embeddings
â”œâ”€â”€ qa_generator/             # Generador de datasets Q&A
â”‚   â”œâ”€â”€ qa_generator.py       # Generador principal
â”‚   â”œâ”€â”€ process_all_chunks_v4.py # Procesamiento masivo
â”‚   â”œâ”€â”€ chunk_manager.py      # GestiÃ³n de chunks
â”‚   â”œâ”€â”€ prompt_templates.py   # Templates de preguntas
â”‚   â”œâ”€â”€ quality_evaluator.py  # Evaluador de calidad
â”‚   â””â”€â”€ qa_dataset/          # Datasets generados
â”œâ”€â”€ scripts/                  # Scripts ejecutables principales
â”‚   â”œâ”€â”€ init_system.py        # InicializaciÃ³n del sistema
â”‚   â”œâ”€â”€ verify_system.py      # VerificaciÃ³n de integridad
â”‚   â”œâ”€â”€ process_manuals_sqlite.py # Procesamiento principal
â”‚   â”œâ”€â”€ build_vectordb_sqlite.py  # ConstrucciÃ³n de vectorDB
â”‚   â”œâ”€â”€ extract_with_manual_folders.py # ExtracciÃ³n organizada
â”‚   â”œâ”€â”€ process_technical_diagrams.py  # Procesamiento de diagramas
â”‚   â”œâ”€â”€ migrate_to_sqlite.py  # MigraciÃ³n de datos legacy
â”‚   â””â”€â”€ generate_qa_dataset.py # GeneraciÃ³n de Q&A
â”œâ”€â”€ utils/                    # Utilidades generales
â”œâ”€â”€ vectorstore/              # GestiÃ³n de almacenamiento vectorial
â”‚   â”œâ”€â”€ vector_manager.py     # Manager principal de vectores
â”‚   â”œâ”€â”€ sqlite_adapter.py     # Adaptador SQLite-ChromaDB
â”‚   â”œâ”€â”€ indexing.py          # Estrategias de indexaciÃ³n
â”‚   â””â”€â”€ retrieval.py         # Mecanismos de recuperaciÃ³n
â”œâ”€â”€ requirements.txt         # Dependencias Python
â”œâ”€â”€ .env.example            # Ejemplo de configuraciÃ³n
â”œâ”€â”€ .gitignore              # Archivos ignorados por git
â”œâ”€â”€ CLAUDE.md               # GuÃ­a para Claude Code
â””â”€â”€ README.md               # Este archivo
```

## ğŸ—„ï¸ Base de Datos SQLite

### Esquema Principal

#### Tabla `documents`
- InformaciÃ³n completa de manuales procesados
- Metadatos extraÃ­dos del nombre de archivo
- Versionado y timestamps

#### Tabla `content_chunks`
- Fragmentos de texto procesados (~512 caracteres)
- Referencias a pÃ¡gina y posiciÃ³n
- Keywords y entidades extraÃ­das
- Embeddings opcionales

#### Tabla `visual_content`
- Metadatos de imÃ¡genes y diagramas
- Hash MD5 para deduplicaciÃ³n
- Dimensiones y tipo de contenido
- Rutas de archivos en filesystem

#### Tabla `structured_tables`
- Metadatos de tablas extraÃ­das
- Estructura y headers
- Rutas a archivos CSV
- EstadÃ­sticas de contenido

#### Tabla `document_analysis`
- AnÃ¡lisis automÃ¡tico de tipo de documento
- MÃ©tricas de contenido
- Recomendaciones de procesamiento

#### Ãndices y Optimizaciones
- Ãndices en campos de bÃºsqueda frecuente
- FTS5 para bÃºsqueda de texto completo
- Triggers para actualizaciÃ³n automÃ¡tica

## âš™ï¸ ConfiguraciÃ³n Avanzada

### Ajuste de ParÃ¡metros de Chunking

En `config/settings.py`:

```python
# TamaÃ±os adaptativos por tipo de documento
ADAPTIVE_CHUNK_SIZES = {
    'technical_diagram_heavy': 1024,  # MÃ¡s contexto para diagramas
    'text_heavy': 512,                # EstÃ¡ndar para texto
    'table_heavy': 768,               # Balance para tablas
    'scanned': 768,                   # MÃ¡s contexto para OCR
    'mixed': 512                      # Por defecto
}

# ConfiguraciÃ³n base
CHUNK_SIZE = 512
CHUNK_OVERLAP = 50
MIN_CHUNK_SIZE = 200  # Chunks menores se descartan
```

### ConfiguraciÃ³n de Modelos de Embeddings

```python
# Modelo multilingÃ¼e optimizado para espaÃ±ol/inglÃ©s
EMBEDDING_MODEL = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
EMBEDDING_DIMENSION = 384

# Alternativas disponibles:
# - "all-MiniLM-L6-v2": MÃ¡s rÃ¡pido, solo inglÃ©s
# - "distiluse-base-multilingual-cased-v2": MÃ¡s preciso, mÃ¡s lento
```

### OptimizaciÃ³n de Rendimiento

```python
# Procesamiento paralelo
MAX_WORKERS = 4  # Ajustar segÃºn CPU disponible
BATCH_SIZE = 32  # Para generaciÃ³n de embeddings

# LÃ­mites de memoria
MAX_IMAGE_SIZE = (3000, 3000)  # Redimensionar imÃ¡genes grandes
MAX_TABLE_ROWS = 1000  # LÃ­mite para tablas muy grandes

# Cache
CACHE_TTL = 86400  # 24 horas para cache de bÃºsqueda
EMBEDDING_CACHE_SIZE = 10000  # NÃºmero mÃ¡ximo de embeddings en cache
```

## ğŸ“Š EstadÃ­sticas y Monitoreo

### Ejemplo de Procesamiento

Para 3 manuales tÃ©cnicos tÃ­picos:

```
RESUMEN DE PROCESAMIENTO POR LOTES
============================================================
Total de manuales: 3
Exitosos: 3 (100.0%)

EstadÃ­sticas agregadas:
  - PÃ¡ginas procesadas: 892
  - Chunks generados: 4,558
  - ImÃ¡genes extraÃ­das: 743
  - Diagramas renderizados: 868
  - Tablas procesadas: 1,244
  
Tiempo total: 48 minutos
Velocidad promedio: 18.5 pÃ¡ginas/minuto
Tokens estimados: ~630,000
```

### Monitoreo en Tiempo Real

```bash
# Ver progreso de procesamiento
tail -f data/logs/processing/manual_*.json | jq '.'

# EstadÃ­sticas de base de datos
sqlite3 data/sqlite/manuals.db "
  SELECT 
    COUNT(DISTINCT document_id) as manuals,
    COUNT(*) as total_chunks,
    AVG(LENGTH(content)) as avg_chunk_size
  FROM content_chunks;
"

# Estado de vectorDB
python scripts/build_vectordb_sqlite.py --stats
```

## ğŸ” API de BÃºsqueda (Ejemplo)

```python
from vectorstore.retrieval import AdvancedRetrieval
from config.settings import get_config

# Inicializar sistema de bÃºsqueda
config = get_config()
retrieval = AdvancedRetrieval(config)

# BÃºsqueda hÃ­brida
results = retrieval.hybrid_search(
    query="conexiÃ³n servo drive AX5000",
    k=10,
    filters={
        "manufacturer": "Beckhoff",
        "doc_type": "technical"
    },
    search_type="hybrid"  # vectorial + keyword
)

# Procesar resultados
for result in results:
    print(f"Score: {result['score']:.3f}")
    print(f"Manual: {result['metadata']['manual_name']}")
    print(f"PÃ¡gina: {result['metadata']['page_num']}")
    print(f"Contenido: {result['content'][:200]}...")
    print("-" * 50)
```

## ğŸ› SoluciÃ³n de Problemas

### Error: "UNIQUE constraint failed"
```bash
# Usar opciÃ³n de reprocesamiento para actualizar
python scripts/process_manuals_sqlite.py --reprocess [manual_id] --steps [pasos]
```

### Error: "No module named 'jpype'"
- Es una advertencia de Tabula, no afecta el funcionamiento
- Instalar jpype1 si se requiere mejor rendimiento: `pip install jpype1`

### Error de memoria con PDFs grandes
```python
# En config/settings.py reducir:
BATCH_SIZE = 16  # De 32 a 16
MAX_WORKERS = 2  # De 4 a 2
PAGE_BATCH_SIZE = 10  # Procesar menos pÃ¡ginas a la vez
```

### ImÃ¡genes CMYK no se visualizan
- LimitaciÃ³n conocida de algunas librerÃ­as
- El sistema las detecta y omite automÃ¡ticamente
- Los diagramas renderizados compensan esta limitaciÃ³n

### Rate limiting en generaciÃ³n Q&A
```bash
# Ajustar parÃ¡metros de rate limiting
python qa_generator/process_all_chunks_v4.py \
    --rps 1 \              # Reducir requests por segundo
    --batch-size 2 \       # Reducir tamaÃ±o de lote
    --model gpt-3.5-turbo  # Usar modelo con lÃ­mites mÃ¡s altos
```

## ğŸ“ˆ Rendimiento y OptimizaciÃ³n

### Cache de Embeddings
- **TecnologÃ­a**: LMDB (Lightning Memory-Mapped Database)
- **UbicaciÃ³n**: `data/cache/embedding_cache/`
- **Beneficio**: Reduce tiempo de reprocesamiento ~80%
- **TamaÃ±o tÃ­pico**: 500MB-2GB segÃºn volumen

### Procesamiento Paralelo
- ExtracciÃ³n de texto: Hasta 4x mÃ¡s rÃ¡pido
- GeneraciÃ³n de embeddings: Procesamiento por lotes
- Renderizado de diagramas: Pool de workers

### Almacenamiento Eficiente
- **SQLite**: Consultas de metadatos en <10ms
- **ChromaDB**: BÃºsqueda vectorial optimizada con HNSW
- **Filesystem**: Acceso directo a binarios grandes

## ğŸš¦ Roadmap

### En Desarrollo
- [ ] API REST completa para integraciÃ³n
- [ ] Interfaz web de bÃºsqueda y visualizaciÃ³n
- [ ] Soporte para mÃ¡s formatos (DOCX, HTML, EPUB)
- [ ] Pipeline de fine-tuning para modelos especÃ­ficos

### PrÃ³ximas CaracterÃ­sticas
- [ ] IntegraciÃ³n con LLMs locales (Ollama, llama.cpp)
- [ ] Dashboard de analytics y mÃ©tricas
- [ ] ExportaciÃ³n de conocimiento a formatos estÃ¡ndar
- [ ] Versionado automÃ¡tico de documentos
- [ ] Clustering semÃ¡ntico de contenido
- [ ] DetecciÃ³n de cambios entre versiones

## ğŸ¤ Contribuir

1. Fork el repositorio
2. Crear rama para tu feature (`git checkout -b feature/NuevaCaracteristica`)
3. Realizar cambios y tests
4. Commit con mensaje descriptivo (`git commit -m 'Add: Nueva caracterÃ­stica X'`)
5. Push a la rama (`git push origin feature/NuevaCaracteristica`)
6. Crear Pull Request con descripciÃ³n detallada

### GuÃ­as de ContribuciÃ³n
- Seguir PEP 8 para estilo de cÃ³digo Python
- Agregar docstrings a todas las funciones pÃºblicas
- Incluir tests para nuevas caracterÃ­sticas
- Actualizar documentaciÃ³n relevante

## ğŸ“ Licencia

Este proyecto estÃ¡ licenciado bajo [especificar licencia].

## ğŸ’¬ Soporte

### Canales de Soporte
- **Issues**: Reportar bugs o solicitar caracterÃ­sticas en GitHub
- **DocumentaciÃ³n**: Wiki del proyecto para guÃ­as detalladas
- **Ejemplos**: Carpeta `examples/` con casos de uso

### Preguntas Frecuentes
- **Â¿Soporta PDFs protegidos?** No, deben estar desprotegidos
- **Â¿LÃ­mite de tamaÃ±o de PDF?** Recomendado <100MB por archivo
- **Â¿Puedo usar mis propios embeddings?** SÃ­, configurable en settings.py

## ğŸ™ Agradecimientos

- **PyMuPDF** - ExtracciÃ³n robusta de contenido PDF
- **ChromaDB** - Base de datos vectorial de alto rendimiento  
- **Sentence Transformers** - Modelos de embeddings multilingÃ¼es
- **Tabula-py / pdfplumber** - ExtracciÃ³n precisa de tablas
- **Tesseract OCR** - Reconocimiento Ã³ptico de caracteres
- **LangChain** - Framework para aplicaciones LLM
- **OpenAI** - APIs para generaciÃ³n de Q&A

---

*Desarrollado para el procesamiento eficiente de documentaciÃ³n tÃ©cnica industrial con enfoque en manuales de automatizaciÃ³n y control.*